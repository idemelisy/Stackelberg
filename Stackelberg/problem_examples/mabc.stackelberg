agents: 2
discount: 1.0
values: reward

states: 0 1 2 3
actions: 
0 1
0 1
observations:
0 1
0 1

start: 0.0 0.0 0.0 1.0

# Transition Dynamics
T: 0 0 : 0 : 0 : 0.09
T: 0 0 : 1 : 0 : 0.09
T: 0 0 : 2 : 0 : 0.09
T: 0 0 : 3 : 0 : 0.09
T: 0 0 : 0 : 2 : 0.81
T: 0 0 : 1 : 2 : 0.81
T: 0 0 : 2 : 2 : 0.81
T: 0 0 : 3 : 2 : 0.81
T: 0 0 : 0 : 1 : 0.01
T: 0 0 : 1 : 1 : 0.01
T: 0 0 : 2 : 1 : 0.01
T: 0 0 : 3 : 1 : 0.01
T: 0 0 : 0 : 3 : 0.09
T: 0 0 : 1 : 3 : 0.09
T: 0 0 : 2 : 3 : 0.09
T: 0 0 : 3 : 3 : 0.09

T: 0 1 : 3 : 3 : 0.9
T: 0 1 : 3 : 1 : 0.1

T: 0 1 : 1 : 3 : 0.9
T: 0 1 : 1 : 1 : 0.1

T: 0 1 : 2 : 3 : 0.09
T: 0 1 : 2 : 1 : 0.01
T: 0 1 : 2 : 2 : 0.81
T: 0 1 : 2 : 0 : 0.09

T: 0 1 : 0 : 3 : 0.09
T: 0 1 : 0 : 1 : 0.01
T: 0 1 : 0 : 2 : 0.81
T: 0 1 : 0 : 0 : 0.09

T: 1 0 : 3 : 3 : 0.1
T: 1 0 : 3 : 2 : 0.9
T: 1 0 : 2 : 3 : 0.1
T: 1 0 : 2 : 2 : 0.9
T: 1 0 : 1 : 3 : 0.09
T: 1 0 : 1 : 2 : 0.81
T: 1 0 : 0 : 3 : 0.09
T: 1 0 : 0 : 2 : 0.81
T: 1 0 : 1 : 1 : 0.01
T: 1 0 : 1 : 0 : 0.09
T: 1 0 : 0 : 1 : 0.01
T: 1 0 : 0 : 0 : 0.09
T: 1 1 : 3 : 3 : 1.0
T: 1 1 : 1 : 3 : 0.9
T: 1 1 : 1 : 1 : 0.1
T: 1 1 : 2 : 3 : 0.1
T: 1 1 : 2 : 2 : 0.9
T: 1 1 : 0 : 3 : 0.09
T: 1 1 : 0 : 1 : 0.01
T: 1 1 : 0 : 2 : 0.81
T: 1 1 : 0 : 0 : 0.09

# Observations
O: 0 0 : 0 : 0 0 : 0.9
O: 0 0 : 0 : 1 0 : 0.1
O: 0 0 : 0 : 0 1 : 0.1
O: 0 0 : 0 : 1 1 : 0.8

O: 1 0 : 1 : 0 0 : 0.8
O: 1 0 : 1 : 1 0 : 0.2
O: 1 0 : 1 : 0 1 : 0.2
O: 1 0 : 1 : 1 1 : 0.6

O: 0 1 : 2 : 0 0 : 0.85
O: 0 1 : 2 : 1 0 : 0.15
O: 0 1 : 2 : 0 1 : 0.15
O: 0 1 : 2 : 1 1 : 0.7

O: 1 1 : 3 : 0 0 : 0.7
O: 1 1 : 3 : 1 0 : 0.3
O: 1 1 : 3 : 0 1 : 0.3
O: 1 1 : 3 : 1 1 : 0.4

# Rewards
R: 0 0 : 0 : * : * : 0 0
R: 0 0 : 1 : * : * : 0 0
R: 0 0 : 2 : * : * : 0 0
R: 0 0 : 3 : * : * : 0 0
R: 1 1 : 0 : * : * : 0 0
R: 1 1 : 1 : * : * : 0 0
R: 1 1 : 2 : * : * : 0 0
R: 1 1 : 3 : * : * : 0 0
R: 0 1 : 3 : * : * : 1 1
R: 0 1 : 2 : * : * : 1 1
R: 0 1 : 1 : * : * : 0 0
R: 0 1 : 0 : * : * : 0 0
R: 1 0 : 3 : * : * : 1 1
R: 1 0 : 2 : * : * : 0 0
R: 1 0 : 1 : * : * : 1 1
R: 1 0 : 0 : * : * : 0 0