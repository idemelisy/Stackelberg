agents: 2
discount: 1.0
value: rewards

states: 0 1
actions:
0 1
0 1
observations:
0 1
0 1
start: 1.0 0.0

# Transition Dynamics
T: 0 1 : 1 : 1 : 0.5
T: 0 1 : 1 : 0 : 0.5
T: 0 1 : 0 : 1 : 0.5
T: 0 1 : 0 : 0 : 0.5
T: 0 0 : 1 : 1 : 0.5
T: 0 0 : 1 : 0 : 0.5
T: 0 0 : 0 : 1 : 0.5
T: 0 0 : 0 : 0 : 0.5
T: 1 0 : 1 : 1 : 0.5
T: 1 0 : 1 : 0 : 0.5
T: 1 0 : 0 : 1 : 0.5
T: 1 0 : 0 : 0 : 0.5
T: 1 1 : 1 : 1 : 0.5
T: 1 1 : 1 : 0 : 0.5
T: 1 1 : 0 : 1 : 0.5
T: 1 1 : 0 : 0 : 0.5

# Observation Probabilities
O: 0 1 : 1 : 1 1 : 0.25
O: 0 1 : 1 : 1 0 : 0.25
O: 0 1 : 1 : 0 0 : 0.25
O: 0 1 : 1 : 0 1 : 0.25
O: 0 1 : 0 : 1 1 : 0.25
O: 0 1 : 0 : 1 0 : 0.25
O: 0 1 : 0 : 0 0 : 0.25
O: 0 1 : 0 : 0 1 : 0.25
O: 0 0 : 1 : 1 1 : 0.25
O: 0 0 : 1 : 1 0 : 0.25
O: 0 0 : 1 : 0 0 : 0.25
O: 0 0 : 1 : 0 1 : 0.25
O: 0 0 : 0 : 1 1 : 0.25
O: 0 0 : 0 : 1 0 : 0.25
O: 0 0 : 0 : 0 0 : 0.25
O: 0 0 : 0 : 0 1 : 0.25
O: 1 0 : 1 : 1 1 : 0.25
O: 1 0 : 1 : 1 0 : 0.25
O: 1 0 : 1 : 0 0 : 0.25
O: 1 0 : 1 : 0 1 : 0.25
O: 1 0 : 0 : 1 1 : 0.25
O: 1 0 : 0 : 1 0 : 0.25
O: 1 0 : 0 : 0 0 : 0.25
O: 1 0 : 0 : 0 1 : 0.25
O: 1 1 : 1 : 1 1 : 0.25
O: 1 1 : 1 : 1 0 : 0.25
O: 1 1 : 1 : 0 0 : 0.25
O: 1 1 : 1 : 0 1 : 0.25
O: 1 1 : 0 : 1 1 : 0.25
O: 1 1 : 0 : 1 0 : 0.25
O: 1 1 : 0 : 0 0 : 0.25
O: 1 1 : 0 : 0 1 : 0.25

# Rewards
R: 0 0 : 0 : * : * : -50 -50
R: 0 1 : 0 : * : * : -100 -100
R: 1 0 : 0 : * : * : -100 -100
R: 1 1 : 0 : * : * : 20 20
R: 0 0 : 1 : * : * : 20 20
R: 0 1 : 1 : * : * : -100 -100
R: 1 0 : 1 : * : * : -100 -100
R: 1 1 : 1 : * : * : -50 -50